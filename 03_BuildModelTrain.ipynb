{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bf7147d",
   "metadata": {},
   "source": [
    "# 1.0 Processing and Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baed935",
   "metadata": {},
   "source": [
    "### 1.1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233c139a",
   "metadata": {},
   "source": [
    "Image super-resolution (SR) is the process of recovering high-resolution (HR) images from low-resolution (LR) images. It is an important class of image processing techniques in computer vision and image processing and enjoys a wide range of real-world applications, such as medical imaging, satellite imaging, surveillance and security, astronomical imaging, amongst others.\n",
    "\n",
    "Image super-resolution (SR) problem, particularly single image super-resolution (SISR), has gained a lot of attention in the research community. SISR aims to reconstruct a high-resolution image ISR from a single low-resolution image ILR. Generally, the relationship between ILR and the original high-resolution image IHR can vary depending on the situation. Many studies assume that ILR is a bicubic downsampled version of IHR, but other degrading factors such as blur, decimation, or noise can also be considered for practical applications. Interpolation-based methods such as nearest-neighbor, bilinear, splines, etc., often introduce some side effects such as computational complexity, noise amplification, blurring results, etc., that are no longer the factors in deep-learning based non-linear methods to achieve super resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a32385",
   "metadata": {},
   "source": [
    "### 1.2 Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfcb16b",
   "metadata": {},
   "source": [
    "Electron microscopy images are very noisy. As the resolution increases, signal to noise ratio (SNR) becomes poorer. In addition, loger exposure to reduce the noise is not feasible due to stage movement that introduces motion blur in the image. To avoid the motion blur, conventional approach is to collect many images with a shortest pssible exposure time where the stage motion effect can be ignored and register those images with some correlation method and average the pixels that are common to all the images. This averaged image constitue to one single image with high SNR. Unfortuntely, multiple exposure of the specimens to electron beam damages the specimen. Each short exposure image frames may differ in statistical features although they are imaging the same specimen. This degrades the image quality.\n",
    "Is there a way to reduce the noise level in short exposure image frames and use only a few imge frames to average that can significantly increase the SNR of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2454a903",
   "metadata": {},
   "source": [
    "There is (in general) no relationship between the noise in neighboring pixels. Technical junkies call this “no correlation”. Correlation is the long-term average of the product of two signals N1 x N2. If two signals have no correlation, then the mean of their product is zero. The signal in neighboring pixels has a high degree of correlation. If you add uncorrelated signals, then their “power” is added, meaning the combined signal is the square root of the combined power.\n",
    "\n",
    "N_comb = sqrt(N1^2+N2^2) and for N1 = N2 = N we get N_comb = sqrt(2)*N, where N1, N2 are root-mean-square (RMS) values of the noise.\n",
    "\n",
    "However, if signals are highly correlated, then their sum is effectively the sum of their magnitudes:\n",
    "\n",
    "S_comb = S1+S2 and for S1=S2=S we get S_comb = 2*S\n",
    "\n",
    "So, if we add the content of two neighboring pixels, we get:\n",
    "\n",
    "SNR_comb = S_comb/N_comb = sqrt(2)*(S/N)\n",
    "\n",
    "So, the signal-to-noise increases by square root of two, which is about 40%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e628b0",
   "metadata": {},
   "source": [
    "We propose deep-learning encoder-decoder architecture to reduce the noise in individual short exposure time images (called image frames) while increasing the resolution by 4X upsampling. This significantlt reduces the noise and increases the SNR but is hard to mathematically quantify. The final image is down-sampled with conventional methods where every 2X down sampling increases the SNR by 40% as per above known theory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d088785",
   "metadata": {},
   "source": [
    "### 2.0 Deep-learning processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21a1f842",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load general libaries to be used\n",
    "import numpy as np\n",
    "import glob\n",
    "import skimage.io\n",
    "import skimage.exposure\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "996837c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load deep-learning libraries such as tensorflow and the easy to use wrappers for tensorflow methods using keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input, regularizers\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, UpSampling2D, Add, Dropout, Lambda\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5a8254f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\myData\\ML_Quality\\4X\\x_train_highres_4X\\*.png\n",
      "D:\\myData\\ML_Quality\\4X\\y_train_highres_4X\\*.png\n"
     ]
    }
   ],
   "source": [
    "path = \"D:\\\\myData\\\\ML_Quality\\\\4X\"\n",
    "x_train_dataPath = path+\"\\\\x_train_highres_4X\\\\*.png\"\n",
    "print(x_train_dataPath)\n",
    "y_train_dataPath = path+\"\\\\y_train_highres_4X\\\\*.png\"\n",
    "print(y_train_dataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc94cead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7002\n",
      "5251\n",
      "1751\n",
      "(128, 128)\n",
      "(128, 128)\n"
     ]
    }
   ],
   "source": [
    "# Read training image pairs\n",
    "files = glob.glob(x_train_dataPath, recursive=True)\n",
    "print(len(files))\n",
    "\n",
    "x_train=[]\n",
    "x_valid=[]\n",
    "ct=0;\n",
    "for file in files: \n",
    "    img = skimage.io.imread(file)\n",
    "    img = np.array(img)\n",
    "    if ct%4 ==0:\n",
    "        x_valid.append(img)\n",
    "        ct=ct+1\n",
    "    else:\n",
    "        x_train.append(img)\n",
    "        ct=ct+1\n",
    "        \n",
    "\n",
    "print(len(x_train))\n",
    "print(len(x_valid))\n",
    "\n",
    "print(x_train[0].shape)\n",
    "print(x_valid[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8abdb9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7002\n",
      "5251\n",
      "1751\n",
      "5251\n",
      "1751\n",
      "(128, 128)\n",
      "(128, 128)\n",
      "(512, 512)\n",
      "(512, 512)\n"
     ]
    }
   ],
   "source": [
    "ct=0\n",
    "\n",
    "files = glob.glob(y_train_dataPath)\n",
    "print(len(files))\n",
    "\n",
    "### for noise reducer\n",
    "xy_train=[]\n",
    "xy_valid=[]\n",
    "### for super resolution\n",
    "y_train=[]\n",
    "y_valid=[]\n",
    "\n",
    "for file in files: \n",
    "    img = skimage.io.imread(file)         \n",
    "    img = np.array(img)\n",
    "    if ct%4 ==0:\n",
    "        ct=ct+1\n",
    "        y_valid.append(img)\n",
    "        imgx = cv2.resize(img, x_train[0].shape, interpolation=cv2.INTER_AREA)\n",
    "        xy_valid.append(imgx)\n",
    "    else:\n",
    "        y_train.append(img)\n",
    "        imgx = cv2.resize(img, x_train[0].shape, interpolation=cv2.INTER_AREA)\n",
    "        xy_train.append(imgx)\n",
    "        ct=ct+1\n",
    "        \n",
    "print(len(xy_train))\n",
    "print(len(xy_valid))\n",
    "print(len(y_train))\n",
    "print(len(y_valid))\n",
    "print(xy_train[0].shape)\n",
    "print(xy_valid[0].shape)\n",
    "print(y_train[0].shape)\n",
    "print(y_valid[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64b254e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some rearranging of data dimensions, etc.. to suit tensorflow requirements\n",
    "x_train = np.dstack(x_train)\n",
    "x_train = np.rollaxis(x_train,-1)\n",
    "x_valid = np.dstack(x_valid)\n",
    "x_valid = np.rollaxis(x_valid,-1)\n",
    "xy_train = np.dstack(xy_train)\n",
    "xy_train = np.rollaxis(xy_train,-1)\n",
    "xy_valid = np.dstack(xy_valid)\n",
    "xy_valid = np.rollaxis(xy_valid,-1)\n",
    "y_train = np.dstack(y_train)\n",
    "y_train = np.rollaxis(y_train,-1)\n",
    "y_valid = np.dstack(y_valid)\n",
    "y_valid = np.rollaxis(y_valid,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "54a2a004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5251, 128, 128, 1)\n",
      "(5251, 128, 128, 1)\n",
      "(5251, 512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "###We also have to reshape our NumPy array as the current shape of the datasets is (Ntrain, X, Y, 1) and (Nvalid, X, Y, 1). \n",
    "###We need to add a fourth dimension with a single value (e.g., from (Ntrain, X, Y,1) to (Ntrain, X, Y, 1, 1));\n",
    "x_train = x_train[:,:,:, tf.newaxis]\n",
    "x_valid = x_valid[:,:,:, tf.newaxis]\n",
    "xy_train = xy_train[:,:,:, tf.newaxis]\n",
    "xy_valid = xy_valid[:,:,:, tf.newaxis]\n",
    "y_train = y_train[:,:,:, tf.newaxis]\n",
    "y_valid = y_valid[:,:,:, tf.newaxis]\n",
    "\n",
    "print(x_train.shape) ## 1X\n",
    "print(xy_train.shape) ## 1X for noise reducer\n",
    "print(y_train.shape) ## 4X for super resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f4f7bd",
   "metadata": {},
   "source": [
    "### 2.0.1 Encoder-Decoder based noise Reducer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1f47b1",
   "metadata": {},
   "source": [
    "Please note this code is only for demo (epoch=1 is set to show it compiles). Models with large number of epochs were built in office private clouds, compiled and tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a75e45cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165/165 [==============================] - 21s 85ms/step - loss: 1342.9982 - accuracy: 0.0000e+00 - val_loss: 209.2892 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\umesh.adiga\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    }
   ],
   "source": [
    "\n",
    "########## Let us do ML based enhancement here ###################\n",
    "img = Input(shape=(None, None, 1))   \n",
    "############### let’s create the model with an object call ###################\n",
    "#encoding architecture\n",
    "x1 = Conv2D(4, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(img) #64\n",
    "x2 = Conv2D(4, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x1) #64\n",
    "x3 = MaxPool2D(padding='same')(x2)\n",
    "x4 = Conv2D(8, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x3) #128\n",
    "x5 = Conv2D(8, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x4) #128\n",
    "x6 = MaxPool2D(padding='same')(x5)\n",
    "x7 = Conv2D(16, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x6) #128\n",
    "x8 = Conv2D(16, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x7) #128\n",
    "x9 = MaxPool2D(padding='same')(x8)\n",
    "encoded = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x9) #256\n",
    "\n",
    "## decoding architecture\n",
    "y7 = UpSampling2D()(encoded)\n",
    "y8 = Conv2D(16, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(y7) #128\n",
    "y9 = Conv2D(16, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(y8) #128\n",
    "y10 = Add()([x8, y9])\n",
    "y11 = UpSampling2D()(y10)\n",
    "y12 = Conv2D(8, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(y11) #128\n",
    "y13 = Conv2D(8, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(y12) #128\n",
    "y14 = Add()([x5, y13])\n",
    "y15 = UpSampling2D()(y14)\n",
    "y16 = Conv2D(4, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(y15) #64\n",
    "y17 = Conv2D(4, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(y16) #64\n",
    "y18 = Add()([x2, y17])\n",
    "y19 = Conv2D(32, (3, 3), padding='same', kernel_regularizer=regularizers.l1(10e-10))(y18) #32\n",
    "\n",
    "decoded = Conv2D(1, (3, 3), padding='same',activation='relu', kernel_regularizer=regularizers.l1(10e-10))(y19)\n",
    "\n",
    "autoencoder = Model(img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "early_stopper = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=50, verbose=1, mode='min')\n",
    "model_checkpoint =  ModelCheckpoint('noise_reducer_checkpoint.h5', save_best_only = True)\n",
    "\n",
    "#train\n",
    "#epoch=1 is just to show the code works as my home-PC is not capable of running large training datasets through many epochs\n",
    "autoencoder.fit(x_train, \n",
    "                xy_train, \n",
    "                epochs=1, \n",
    "                validation_data=(x_valid, xy_valid),\n",
    "                callbacks=[early_stopper, model_checkpoint])\n",
    "\n",
    "#Actual saved model is from company computing environment where epochs=100 is used to fit the training data\n",
    "autoencoder.save(\"D:\\\\myData\\\\ML_Quality\\\\autoencoder_NR.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49974b36",
   "metadata": {},
   "source": [
    "### 2.0.2 Super-resolution model builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fddb5b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Add, Conv2D, Input, Lambda\n",
    "from tensorflow.python.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0a3caaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    return (x - 127.5) / 127.5\n",
    "def denormalize(x):\n",
    "    return x * 127.5 + 127.5\n",
    "def pixel_shuffle(scale):\n",
    "    return lambda x: tf.nn.depth_to_space(x, scale)\n",
    "def res_block(x_in, filters, scaling):\n",
    "    x = Conv2D(filters, 3, padding='same', activation='relu')(x_in)\n",
    "    x = Conv2D(filters, 3, padding='same')(x)\n",
    "    if scaling:\n",
    "        x = Lambda(lambda t: t * scaling)(x)\n",
    "    x = Add()([x_in, x])\n",
    "    return x\n",
    "def upsample(x, scale, num_filters):\n",
    "    def upsample_1(x, factor, **kwargs):\n",
    "        x = Conv2D(num_filters * (factor ** 2), 3, padding='same', **kwargs)(x)\n",
    "        return Lambda(pixel_shuffle(scale=factor))(x)\n",
    "    if scale == 2:\n",
    "        x = upsample_1(x, 2, name='conv2d_1_scale_2')\n",
    "    elif scale == 3:\n",
    "        x = upsample_1(x, 3, name='conv2d_1_scale_3')\n",
    "    elif scale == 4:\n",
    "        x = upsample_1(x, 2, name='conv2d_1_scale_2')\n",
    "        x = upsample_1(x, 2, name='conv2d_2_scale_2')\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "045ae9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165/165 [==============================] - 207s 1s/step - loss: 483.6505 - accuracy: 1.7383e-04 - val_loss: 374.7721 - val_accuracy: 1.4189e-04\n"
     ]
    }
   ],
   "source": [
    "##Some required parameters that define the deep-learning network\n",
    "num_filters=32\n",
    "num_res_blocks=8\n",
    "res_block_scaling=None\n",
    "scale=4\n",
    "\n",
    "########## Let us do ML based 4X super-resolution enhancement here ###################\n",
    "x_in = Input(shape=(None, None, 1))   \n",
    "x = Lambda(normalize)(x_in)\n",
    "x = b = Conv2D(num_filters, 3, padding='same')(x)\n",
    "for i in range(num_res_blocks):\n",
    "    b = res_block(b, num_filters, res_block_scaling)\n",
    "b = Conv2D(num_filters, 3, padding='same')(b)\n",
    "x = Add()([x, b])\n",
    "x = upsample(x, scale, num_filters)\n",
    "x = Conv2D(3, 3, padding='same')(x)\n",
    "x = Lambda(denormalize)(x)\n",
    "    \n",
    "sr= Model(x_in, x, name=\"sr\")\n",
    "sr.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "early_stopper = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=50, verbose=1, mode='min')\n",
    "model_checkpoint =  ModelCheckpoint('super_resolution_checkpoint.h5', save_best_only = True)\n",
    "\n",
    "#train\n",
    "#epoch=1 is just to show the code works as my home-PC is not capable of running large training datasets through many epochs\n",
    "sr.fit(x_train, y_train, epochs=1, validation_data=(x_valid, y_valid), callbacks=[early_stopper, model_checkpoint])\n",
    "\n",
    "#Actual saved model is from company computing environment where epochs=100 is used to fit the training data\n",
    "autoencoder.save(\"D:\\\\myData\\\\ML_Quality\\\\super_resolution_SR.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6615915a",
   "metadata": {},
   "source": [
    "### 3.0 Summary: Overall architecture or sequence of processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d71b412",
   "metadata": {},
   "source": [
    "Instead of building one complex long deep-learning process, we have divided that into two parts of autoencoder-decoder model to reduce the noise and a super resolution model to increase the resolution by 4X. Thi is deliberately done to keep a greater control on what is happening inside the neural network in case the results are not as expected.\n",
    "In the next report, we use some test data sets that were never seen during the training or validation and analyze the improvement in overall image quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1614bb2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
